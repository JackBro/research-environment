A function turns one number, the argument, into another number, the result. An operator
turns one function into another function. In the vector space representation of a function,
an operator turns one vector into another vector.
Suppose that we are constructing a new function g of y from the function f of x by acting
on f of x with some operator A. And we put the hat over here to indicate the operator,
as usual. The variables x and y might be the same kind of variable, so in the case where
the operator corresponds to differentiation, we're used to this kind of operation d by
the x of f of x gives us some new function, g of x. This is an operator, d by dx, operating
on the function f of x to give us a function g of x.
It's also possible that the variables x and y might be quite different variables. For
example, in the case of a Fourier transform operation where x might represent time and
y might represent frequency. So here's a Fourier transform operation. It's a normalized one,
in this particular case, with the 1 over root 2 pi out the front. And we see that over here,
we have what might be frequency y, and over here, what might be time x.
A standard notation for writing any such operation on a function would just be to say g of y
is equal to A operating on f of x. As I said we should read this not as a multiplication,
but as A operating on f of x.
Now, for A to be the most general operation possible, it should be possible for the value
of g of y, at some specific argument-- y1, for example-- to depend on the values of f
of x for all of the possible values of the argument x. This is the case, for example,
in the Fourier transform operation we just looked at. This is actually a linear operation,
as will become clear. And here we've got g of y being dependent on f of x possibly at
all of its arguments, because we're integrating with some factor in here over all of the possible
values of x.
Now, we are interested here solely in linear operators. They're the only ones we will use
in quantum mechanics, because of the fundamental linearity of quantum mechanics. A linear operator
has the following characteristics-- A operating on the sum of two functions here must be the
same as A operating on each function separately and added together.
And A operating on some number times a function must be the same thing as the number times
A operating on the function. And this should work for any complex number.
Well, let's consider the most general way we could have this function g of y at some
specific value y1 of its argument-- that is, obviously, g of y1-- be related to the values
of f of x for possibly all the values of x and still retain the linearity properties
for this relation. So we want to figure this out. What's the most general way we can write
this down and still preserve linearity?
Well, let's think of the function f of x, as usual, as being represented by a list of
values. So f of x1, the first value of the argument; f of x2; f of x3; and so on, just
as we've done before when considering f of x as a vector. We can take these values of
x to be as closely spaced as we want. We believe that this representation can give us as accurate
a representation of f of x as we need for any calculation we might want to perform.
Then let's propose that for a linear operation, the value g of y1, y1 of its argument here,
might be related to the values of f of x by a relation of this form. So g of y1 would
be some constant, a1 1, times the value of f of x at x1 for its argument plus a1 2 times
f of x2 and so on. And these we would allow to be complex constants.
Now, this form shows the linearity behavior we want, so let's check that. If we replaced
f of x by the sum of two functions, f of x and h of x, then we would have a1 1 into f
of x1 plus h of x1 plus a1 2 into f of x2 plus h of x2 and so on. Well, rather obviously,
we can split that into two sums-- this one here and this one here for the h. And that
means, as required, that this operation here operating on the sum of f of x and h of x
is just the sum of the operation first on f of x and then separately on h of x. That's
one of the criteria we needed for a linear relation.
And in this form, if we replaced f of x by c times f of x, then we would have g of y1
would be a1 1 times c of f of x1, a1 2 times c of f of x2, and so on. But rather trivially,
we see we've got the same factor in all the terms, so we can bring it at the front. So
that's as we required for the second attribute for a linear operator. A operating on c times
f of x has to be c times A operating on f of x. So this relation we have proposed here
is at least satisfying both of the criteria to describe an operation that is linear that
turns f of x at all of its values, possibly, into g at some specific value of its argument.
Now let's consider whether this form is as general as it possibly could be and still
be a linear relation. We can see this by trying to add other powers, or what we'll call cross
terms, of f of x in here. At any more complicated relation that we could have of g of y1 to
f of x at all of its possible values of its argument could presumably be written as some
power series in f of x at all the possible values of its argument.
That might involve f of x for different values of x. That is, cross terms. Somewhere in this
power series, we might have f of x1 times f of x2, for example. That would be the most
general relation we could imagine between g of y1 and all of the possible values of
f of x, would be all of these power series, including all these cross terms.
If we were to add higher powers of f of x, such as f of x squared, or cross terms such
as f of x1 times f of x2 into this series, it would no longer have the required linear
behavior. The key point is that f of x plus h of x all squared is not the same thing as
f of x squared plus h of x squared. There would be cross terms as well on this side
that would not exist on that side, so this would not work.
We also cannot add a constant term to this series. That would violate the second linearity
condition. A operating on c times f of x has to be c times A operating on f of x. If we
added a constant on here, then that additive constant would not itself have been multiplied
by c.
So we can't add higher powers. We similarly can't add cross terms, and we can't add constants.
Hence, we conclude this form of relation is the most general form possible for the relation
between g at y1 and f of x, if this relation is to correspond to a linear operator.
Now let's try to construct the entire operator. To construct the entire function g of y--
that is, at all of its arguments, y-- we should construct series like this for each different
possible value of y1. So for some other value, y2, we would have some other series here--
a2 1 times f of x1 plus a2 2 times f of x2, and so on. So constructing series like this
for each value of y, well, if we write f of x and g of y as vectors, then we can write
all of these series at once by using a matrix.
So now we see that this first row here, g of y1, is indeed the result of taking this
column vector and multiplying by this set of coefficients that's the first row of the
matrix. And similarly, g of y2 would then be the result of multiplying this column vector
into the second row of this matrix, and so on. So now we're saying we can write the most
possible general linear relation between f of x and g of y by writing it in this matrix
form with this matrix of coefficients. A matrix can represent the most general possible linear
operator.
So we see that this whole expression here can be rewritten in a shorthand form as g
of y is equal to A operating on f of x, where the operator A can be written as a matrix.
Here's the operator A now. This is the operator A, and it's a matrix.
Presuming functions can be represented as vectors, then linear operators can be represented
by matrices. In bra-ket notation, we can write this expression out as this one here. So this
is the bra-ket version of that expression there. The ket vector g is equal to A times
the ket vector f. If we regard the ket as a vector, we now regard this linear operator,
A, a matrix.
So when you see an expression like this, you can think of this f as a column vector, this
A as a matrix, and this g here as a column vector. We can always think of the bra-ket
notation in that way.
In the language of vector or function spaces, the operator takes one vector or function
and turns it into another one. All of the following linear mathematical operations can
be described this way-- differentiation, rotation or dilatation of a vector-- stretching of
a vector-- all linear transforms, such as Fourier transforms or Laplace transforms or
maybe Z transforms, convolutions, things like Green's functions and integral equations,
and, in fact, linear integral equations generally.
In quantum mechanics, such linear operators are used as operators associated with measurable
variables, such as the Hamiltonian operator for energy and the momentum operator for momentum.
They're also used as operators corresponding to changing the representation of a function--
that is, changing the basis-- and for a few other specific purposes, with the associated
vectors representing quantum mechanical states.
And every time we work with these operators in this bracket notation, we can think of
them as matrices with all the algebraic properties that make matrices so useful in general. We're
going to look at linear operators as matrices quite explicitly in the next section.