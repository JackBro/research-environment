So far, we've introduced mathematical vectors, vectors at least in the sense we think of
vectors when we write them as columns or rows of numbers. Of course, the idea of a vector
suggests, at least informally, the idea of some space, perhaps analogous in some way
to ordinary geometrical space in which we have ordinary geometrical vectors.
In fact, we can think of these new vectors as being in some space, and it's actually
rather important, mathematically, that we define the properties of this space. We're
going to find that it has much in common with our ideas of geometrical space, which means
that at least loosely, we can visualize much of the mathematics by analogy back to our
ideas of ordinary geometrical space or what we sometimes call Cartesian space.
We can't take this analogy very rigorously for two reasons. First, our mathematical space
here may have a different number of dimensions. This could be fewer than three. For example,
the mathematical space to describe electron spin has only two dimensions. Or it could
be more than three dimensions. In fact, it could even have an infinite number of dimensions.
The second difference is that in the space that we need for representing our quantum
mechanical vectors, we have to allow complex numbers as the projections of some vector
along an axis. Ordinary geometrical space only has real numbers for the projections
of a vector along an axis. However, these differences need not prevent us from informally
thinking of our new vectors as being, in many ways, similar to our existing ideas of geometrical
vectors.
And this can be a useful way to visualize quantum mechanical states, one that will remain
informally useful as we work with systems well beyond the simple functions we've been
discussing so far. Our new space, incidentally, rather than being called a geometrical space,
will be called a Hilbert space. We will start to discuss all of these ideas in more detail
now.
So we need a space, then, in which our vectors can exist. For a vector that had three components,
a one, a two, and a 3, then we can imagine a simple three dimensional Cartesian space.
The vector can be visualized as a line, just like drawing a vector as we're used to doing.
And it would start at the origin. And it would have projected lengths, a1 along the x-direction,
a2 along the y-direction, and a3 along the z-direction, along these axes. And of course,
each of these axes, x, y, and z, these axes are at right angles to one another.
For a function expressed as its value at a set of points, then instead of just three
axes labeled x, y, and z, we might have an infinite number of orthogonal axes and they
would be labeled with their associated basis functions. For example, the basis function
psi n, where that's some particular basis function. Just as we label our axes in conventional
space with unit vectors, for example our notation that we could use here might be this x hat
is a unit vector in the x-direction, y hat is a unit vector in the y-direction, and z
hat is a unit vector in the z-direction. Then, in our space here, we're going to label our
axes with the ket vectors psi n. So maybe one of them is psi 1, another is psi 2, and
so on. Either notation is acceptable for talking about the spaces, either this one or the one
with the kets here.
Now, a very important point is this idea of an inner product, and geometrical space has
the vector dot product. That defines both the orthogonality of the axes. So if two axes
orthogonal, then x, the unit vector along one of them, dot y, the unit vector along
the other, is 0. So for two vectors that are non-zero themselves but have a 0 dot product,
we say they are orthogonal.
And the components of a vector along these axes are also defined using the dot product.
So if we have a vector f, then we have components fx along the x-direction, fy along the y-direction,
and fz along the z-direction. Then, the component fx is simply the dot product of f with the
unit vector along that particular axis. And similarly for the other components, we're
finding these fy's and fz's that are done with the dot product.
Now, our vector space here that we're going to use for our functions has an inner product
that also defines both the orthogonality of the basis functions. So here would be the
condition for orthogonality, this is our inner product of this bra vector, psi m, with this
ket vector, psi n, because these are an orthonormal basis. Then this is one if n is equal to m,
and 0 otherwise. So that's a chronic on delta here. So our vector space is using this inner
product to define the idea of orthogonality, and it also defines the components for us.
So the specific expansion coefficient cm is simply the inner product of the bra vector
psi m with the function f in our expansion with these other expansion coefficients.
So now let's look at the mathematical properties of our space compared to the geometrical vector
space. For example, we can start by looking at addition of vectors. Well, with respect
to addition of vectors, both geometrical space and our vector space are commutative. That
is, the two vectors a plus b added up in that order give us the same thing as the two vectors
added up in the other order b plus a. That's a normal property of vectors and geometrical
space.
Similarly here, if we add two ket vectors, it doesn't matter what order we add them in.
And both of these are associative. If we are adding three vectors, it doesn't matter if
we add b plus c first and then add a to the result of that, or if we add a plus b first
and then c to the result of that. And similarly here, if we had to three ket vectors, f, g,
and h, it doesn't matter if we add g and h first before we f to them. And similarly,
we could add f plus g first and then h to that.
Both of these spaces are linear, so they're linear in multiplying by constants. So a constant
number multiplying the sum of these two vectors is the same as that constant multiplying this
first vector plus that constant multiplying the second vector. So that's one of our linearity
criteria. And similarly here in our new space, multiplying the sum of these two vectors,
these ket vectors f and g, by some number c, is the same thing as multiplying the vectors
individually, f multiplied by c plus g multiplied by c. And one subtlety here is that our constant
c are going to be allowed to be complex numbers, not just real ones
And our inner product is linear, both in multiplying by constants. So here is a dot product into
c times b. That's the same thing as c into e dot b. And similarly, with our bra and kets
in our new space, the inner product of f with the ket vector c times d is the same thing
as c times the inner product of f with g. And in superposition, so a dot b plus c is
the same thing as a dot b plus a dot c. And in our case with the bras and kets, f inner
producted with the sum of g plus h is the same thing as the sum of the inner products
f inner producted with g and f inner producted with h.
There is a well defined length to the vector, and that turns out to be a rather important
concept that we must have in our space. So, an ordinary geometrical space, this length,
or what we sometimes call a norm, is just the square root of the dot product of the
vector with itself. Sometimes in mathematics, that's indicated with this double lined notation.
Similarly here, we have the inner product of the vector with itself. We take the square
root of that to get the norm of f.
This may seem an obvious property, but it's very important in a vector space like the
one we have that it has an idea of length in it. Because the idea of length means we
know how far something is from something else. For example if we're trying to find an approximation
to some function, we need to know formally how close we might be to the right answer,
and we need to be able to define that as some sort of distance or norm. That's critical
for most of the underlying mathematical proofs of how these spaces work.
Now, a very important concept that we've already discussed is the idea of completeness. And
in both cases or both our species here, any vector in this space can be represented to
an arbitrary degree of accuracy as a linear combination of the basis vectors. This is
the idea of completeness, as I said, of the basis set.
In vector spaces this property of the vector space itself is sometimes described as compactness.
This idea of compactness is one that has several slightly different uses in the formal mathematics
of vector spaces, and we won't end up discussing it in much more detail, but this term does
occur.
In geometrical space, the lengths ax, ay, and az of the vector's components are real.
So the inner product of the vector dot product, in this case, is commutative. A dot b is equal
to b dot a. But with complex coefficients rather than real lengths, we choose a non-commutative
inner product in the following sense. The inner product of f with g is going to be the
complex conjugate of the inner product of g with f.
So in that sense, the inner product is not commutative. The inner product of f and g
is not equal to the inner product of g and f. It's equal to the complex conjugate of
it. So that's the first important and somewhat significant difference between our vector
space and the ordinary geometrical vector space. And it's a consequence of the fact
that we want to be able to work with complex numbers.
This relation turns out to be a very important and useful one. For example, it ensures that
the inner product of f with itself has to be real, because it has to equal its own complex
conjugate. But even if we're working with complex numbers, this inner product of f with
itself is always a real number because of this property of the inner product.
And the fact that it is a real means it can be used to form a useful norm. So a mathematician,
in constructing this space, might say, let's construct a space like this, where we make
the inner product, when we flip it around, be a complex conjugate. That means that we
get all these nice mathematical properties in our space. And of course, this space is
going to turn out to be a very good one to represent all of our quantum mechanics.
The elementary mathematical properties above, other than the inner product, are sufficient
to define these two spaces as linear vector spaces. Mathematically, our vector space can
also be called a function space. A vector in this space is a representation of a function.
The set of bases vectors or bases functions that can be used to represent vectors in this
space is said, in linear algebra, to span the space.
With the properties of the inner product, these are what are called Hilbert spaces.
The Hilbert space is the space in which the vector representation of the function exists,
just as normal Cartesian geometrical space is the space in which a geometrical vector
exists.
The main differences between our vector space and geometrical space are, first of all, our
components can be complex numbers rather than only real one. Second, we can have more dimensions,
possibly an infinite number. third, because we are working with complex numbers, our definition
of inner product has to be slightly different from the vector dot product that is the inner
product in ordinary geometrical space.
However that inner product still helps us define the idea of orthogonality. If the vector
dot product of two nonzero vectors and the geometrical space is 0, then we say that those
vectors are orthogonal. That is, at right angles in that space. Similarly, if our inner
product of two nonzero vectors in our vector space is 0, then we will say that those two
vectors are also orthogonal.
As we said, despite these differences, the idea of a geometrical space is still a useful
and formal starting point for visualizing our vector space, and we will continue to
use that to help us specialize quantum mechanics.